Ğ’Ğ¾Ñ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM:

text
# MCP Server & Chat Integration Project with RAG + Git + Support + Team Assistant + Local LLM

## ĞĞ±Ñ‰ĞµĞµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ MCP (Model Context Protocol) ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² Ñ React-Ñ‡Ğ°Ñ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, **RAG (Retrieval-Augmented Generation)** ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹, **Support Assistant**, **Team Assistant** Ğ¸ **Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM (Ollama)**. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, GitHub API, Docker, Git Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Document Indexing Pipeline, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¾Ğ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².

## Deployment Architecture

### Production Endpoints
- **Frontend (Vercel)**: `https://your-app.vercel.app`
  - React 18 + Vite
  - Serverless Functions
  - Environment: VITE_API_URL
  
- **Backend (Railway)**: `https://your-backend.railway.app`
  - Node.js Express API
  - Nixpacks builder
  - Root Directory: `/server`
  - Environment: PORT=4000 (Railway Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸)

- **Local LLM (Ollama)**: `http://localhost:11434`
  - Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°
  - ĞœĞ¾Ğ´ĞµĞ»Ğ¸: gemma3:4b, llama3.2:3b, nomic-embed-text
  - Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· REST API

### GitHub Actions CI/CD Pipeline

**Workflow Ñ„Ğ°Ğ¹Ğ»**: `.github/workflows/deploy.yml`

```yaml
name: Deploy to Production

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  deploy-frontend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy to Vercel
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          working-directory: ./client
          vercel-args: '--prod'
          
  deploy-backend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy to Railway
        run: |
          npm install -g @railway/cli
          railway up --service backend
        env:
          RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}
          
  notify:
    needs: [deploy-frontend, deploy-backend]
    runs-on: ubuntu-latest
    steps:
      - name: Discord Notification
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ secrets.DISCORD_WEBHOOK }}
          title: "Deployment Complete"
          description: |
            âœ… Frontend: ${{ secrets.VERCEL_URL }}
            âœ… Backend: ${{ secrets.RAILWAY_URL }}
GitHub Secrets Configuration
Required Secrets (Settings â†’ Secrets and variables â†’ Actions):

Secret Name	Description	Source
VERCEL_TOKEN	Vercel Access Token	vercel.com â†’ Account Settings â†’ Tokens
VERCEL_ORG_ID	Vercel Organization ID	vercel.com/account â†’ Settings â†’ General â†’ Your ID
VERCEL_PROJECT_ID	Vercel Project ID	vercel.com/dashboard â†’ Project â†’ Settings â†’ Project ID
RAILWAY_TOKEN	Railway API Token	railway.app â†’ Account Settings â†’ Tokens
PERPLEXITY_API_KEY	Perplexity AI API Key	perplexity.ai/settings/api
DOCKER_USERNAME	Docker Hub Username	hub.docker.com â†’ Account
DOCKER_PASSWORD	Docker Hub Access Token	hub.docker.com â†’ Account Settings â†’ Security
DISCORD_WEBHOOK	Discord Webhook URL	Discord â†’ Channel Settings â†’ Integrations â†’ Webhooks
VERCEL_URL	Frontend Production URL	ĞŸĞ¾ÑĞ»Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¿Ğ»Ğ¾Ñ
RAILWAY_URL	Backend Production URL	ĞŸĞ¾ÑĞ»Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¿Ğ»Ğ¾Ñ
Ğ¡Ñ‚ĞµĞº Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹
Ğ¯Ğ·Ñ‹Ğº backend: JavaScript (Node.js ESM)

Ğ¯Ğ·Ñ‹Ğº frontend: JavaScript (React 18)

Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸: Express.js, React, Vite

AI/LLM:

Perplexity API (sonar model) - Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ°Ñ

Ollama (gemma3:4b, llama3.2:3b) - Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ

MCP SDK: @modelcontextprotocol/sdk

Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸: Axios, node-fetch, node-cron, dockerode

Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: Docker, Docker Compose, Git

Deployment: Vercel (Frontend), Railway (Backend)

CI/CD: GitHub Actions

Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹
text
AI_Challenge_23/
â”œâ”€â”€ .github/                           # ğŸ†• GitHub Actions CI/CD
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml                 # Deployment pipeline
â”‚
â”œâ”€â”€ server/                            # Backend (Node.js) - Railway
â”‚   â”œâ”€â”€ index.js                       # Express ÑĞµÑ€Ğ²ĞµÑ€ (PORT=4000)
â”‚   â”‚
â”‚   â”œâ”€â”€ MCP Clients & Services
â”‚   â”œâ”€â”€ mcpClient.js
â”‚   â”œâ”€â”€ ragMcpClient.js
â”‚   â”œâ”€â”€ gitMcpClient.js
â”‚   â”œâ”€â”€ supportMcpClient.js
â”‚   â”œâ”€â”€ localLlmClient.js             # ğŸ†• ĞšĞ»Ğ¸ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ollama
â”‚   â”‚
â”‚   â”œâ”€â”€ Service Layer
â”‚   â”œâ”€â”€ ragService.js
â”‚   â”œâ”€â”€ assistantService.js
â”‚   â”œâ”€â”€ supportAssistantService.js
â”‚   â”œâ”€â”€ teamAssistantService.js       # ğŸ”„ ĞĞ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½ (+ Local LLM)
â”‚   â”œâ”€â”€ documentIndexer.js
â”‚   â”‚
â”‚   â”œâ”€â”€ MCP Servers
â”‚   â”œâ”€â”€ documents-mcp.js
â”‚   â”œâ”€â”€ task-mcp-server.js
â”‚   â”œâ”€â”€ git-mcp-server.js              # v1.2.1 (Railway compatible)
â”‚   â”œâ”€â”€ docker-mcp-server.js
â”‚   â”‚
â”‚   â”œâ”€â”€ Orchestration & Utils
â”‚   â”œâ”€â”€ agent-orchestrator.js
â”‚   â”œâ”€â”€ githubTools.js
â”‚   â”œâ”€â”€ githubService.js
â”‚   â”œâ”€â”€ prReviewService.js
â”‚   â”‚
â”‚   â”œâ”€â”€ Data Storage
â”‚   â”œâ”€â”€ documents/                     # Markdown Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
â”‚   â”œâ”€â”€ indexes/                       # JSON Ğ¸Ğ½Ğ´ĞµĞºÑÑ‹ (343 embeddings)
â”‚   â”œâ”€â”€ tasks.json                     # Ğ‘Ğ°Ğ·Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡
â”‚   â”‚
â”‚   â”œâ”€â”€ Configuration
â”‚   â”œâ”€â”€ .env                           # ğŸ”„ Railway + Ollama Environment Variables
â”‚   â”‚   # PERPLEXITY_API_KEY (from Railway secrets)
â”‚   â”‚   # PERPLEXITY_MODEL=sonar
â”‚   â”‚   # OLLAMA_URL=http://localhost:11434  # ğŸ†•
â”‚   â”‚   # OLLAMA_MODEL=gemma3:4b             # ğŸ†•
â”‚   â”‚   # REPO_PATH=/app/repo (Railway volume)
â”‚   â”‚   # PORT=4000 (Railway auto-injected)
â”‚   â”‚
â”‚   â”œâ”€â”€ railway.json                   # ğŸ†• Railway config
â”‚   â”‚   # {"builder": "NIXPACKS", "deploy": {"startCommand": "node index.js"}}
â”‚   â”‚
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ client/                            # Frontend (React + Vite) - Vercel
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatPage.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ AssistantPage.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ SupportPage.jsx
â”‚   â”‚   â”‚   â””â”€â”€ TeamAssistantPage.jsx  # ğŸ”„ ĞĞ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½ (+ LLM Switcher)
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useTeamAssistant.js    # Ğ‘ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹
â”‚   â”‚   â””â”€â”€ styles/
â”‚   â”‚       â””â”€â”€ TeamAssistantPage.css  # ğŸ”„ ĞĞ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½ (+ LLM Switcher styles)
â”‚   â”‚
â”‚   â”œâ”€â”€ .env                           # ğŸ†• Vercel Environment Variables
â”‚   â”‚   # VITE_API_URL=https://your-backend.railway.app
â”‚   â”‚
â”‚   â”œâ”€â”€ vercel.json                    # ğŸ†• Vercel config
â”‚   â”‚   # {"buildCommand": "npm run build", "outputDirectory": "dist"}
â”‚   â”‚
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ Configuration
    â”œâ”€â”€ .env.example                   # ğŸ”„ Ğ¨Ğ°Ğ±Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ + Ollama
    â”œâ”€â”€ .gitignore                     # ğŸ†• Ğ˜ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ .env, node_modules
    â”œâ”€â”€ package.json
    â””â”€â”€ structure.md                   # Ğ­Ñ‚Ğ¾Ñ‚ Ñ„Ğ°Ğ¹Ğ» (v1.3.0)
Deployment Configuration
Railway (Backend)
Ğ¤Ğ°Ğ¹Ğ»: server/railway.json

json
{
  "builder": "NIXPACKS",
  "deploy": {
    "startCommand": "node index.js",
    "healthcheckPath": "/api/health",
    "restartPolicyType": "ON_FAILURE"
  }
}
Environment Variables (Railway Dashboard):

bash
PERPLEXITY_API_KEY=pplx-xxxxxxxxxxxx
PERPLEXITY_MODEL=sonar
REPO_PATH=/app/repo
PORT=4000  # Auto-injected by Railway
NODE_ENV=production
Root Directory: /server (Settings â†’ Source â†’ Root Directory)

Vercel (Frontend)
Ğ¤Ğ°Ğ¹Ğ»: client/vercel.json

json
{
  "buildCommand": "npm run build",
  "outputDirectory": "dist",
  "installCommand": "npm install",
  "framework": "vite",
  "routes": [
    { "src": "/[^.]+", "dest": "/", "status": 200 }
  ]
}
Environment Variables (Vercel Dashboard):

bash
VITE_API_URL=https://your-backend.railway.app
NODE_VERSION=20
Ollama (Local LLM)
Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° (Windows):

bash
# 1. Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ ollama.com
# 2. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ° localhost:11434

# 3. Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
ollama pull gemma3:4b
ollama pull llama3.2:3b
ollama pull nomic-embed-text

# 4. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ
ollama list
curl http://localhost:11434
Environment Variables (Local Development):

bash
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:4b
Development vs Production
Local Development
bash
# Terminal 1 - Ollama (ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸)
ollama serve

# Terminal 2 - Backend
cd server
npm install
npm run dev  # http://localhost:4000

# Terminal 3 - Frontend  
cd client
npm install
npm run dev  # http://localhost:5173
Production (GitHub Actions)
bash
git add .
git commit -m "feat: deploy update"
git push origin main

# ğŸš€ ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´ĞµĞ¿Ğ»Ğ¾Ğ¹:
# 1. GitHub Actions Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ workflow
# 2. Frontend â†’ Vercel (serverless)
# 3. Backend â†’ Railway (container)
# 4. Discord notification Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼
API Endpoints (Production)
Base URL: https://your-backend.railway.app

âœ… Team Assistant API
Endpoint	Method	Body	Description
/api/team/ask	POST	{query, user_id}	Natural Language Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ (Perplexity + Ollama)
ğŸ¤– Local LLM API (NEW)
Endpoint	Method	Body	Description
/api/local-llm/ask	POST	{prompt, temperature, top_p}	ĞŸÑ€ÑĞ¼Ğ¾Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Ollama
/api/local-llm/health	GET	-	ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ollama
/api/local-llm/models	GET	-	Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
ğŸ“š Documents Pipeline
Endpoint	Method	Body	Description
/api/documents/index	POST	{directory, index_name}	Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ğ´ĞµĞºÑ
/api/documents/search	POST	{query, index_name, top_k}	ĞŸĞ¾Ğ¸ÑĞº
ğŸ¤– RAG API
Endpoint	Method	Body	Description
/api/rag/ask	POST	{question, mode, topK}	RAG Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
ğŸ’¬ Support Assistant
Endpoint	Method	Body	Description
/api/support/ask	POST	{user_id, question}	ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°
âœ… Git Assistant API
Endpoint	Method	Body	Description
/api/assistant/command	POST	{command}	Git ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹
Environment Variables
Development (.env.example)
bash
# Perplexity AI
PERPLEXITY_API_KEY=pplx-xxxxxxxxxxxx
PERPLEXITY_MODEL=sonar

# Server
PORT=4000

# Git MCP (Local)
REPO_PATH=D:\perplexity-chat  # Windows: Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ

# Ollama (Local LLM) ğŸ†•
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:4b

# GitHub (optional)
GITHUB_TOKEN=ghp_xxxxxxxxxxxx
Production - Railway (Backend)
bash
PERPLEXITY_API_KEY=<from GitHub Secrets>
PERPLEXITY_MODEL=sonar
REPO_PATH=/app/repo
PORT=4000
NODE_ENV=production
# Ollama Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ² production (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾)
Production - Vercel (Frontend)
bash
VITE_API_URL=https://your-backend.railway.app
NODE_VERSION=20
Team Assistant Features
ğŸŒ Perplexity Mode (Default)
âœ… Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ (ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€, ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ)

âœ… ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Git Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹

âœ… Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°

âœ… Git Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (status, commits, history)

âœ… RAG Ğ¿Ğ¾Ğ¸ÑĞº Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸

âœ… Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸

ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´:

text
"ĞŸĞ¾ĞºĞ°Ğ¶Ğ¸ Ğ²ÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸"
"Ğ§Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼?"
"Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°"
"Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ: Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ±Ğ°Ğ³, Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ high"
"ĞšĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ RAG Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğµ?"
"ĞŸĞ¾ĞºĞ°Ğ¶Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 5 ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚Ğ¾Ğ²"
ğŸ¤– Ollama Mode (Local LLM)
âœ… Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ±ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°

âœ… ĞŸÑ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ (Ğ²ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾)

âœ… ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°

âœ… ĞĞ±Ñ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ

âœ… ĞœĞ¾Ğ´ĞµĞ»Ğ¸: gemma3:4b (3.3 GB), llama3.2:3b (2.0 GB)

ĞĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ:

ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ° "ğŸ¤– Ollama" Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ

Ğ—Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ

ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²:

text
"Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ MCP Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»?"
"ĞĞ±ÑŠÑÑĞ½Ğ¸ ĞºĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ RAG"
"ĞšĞ°Ğº Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Docker ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€?"
"Ğ’ Ñ‡Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ REST Ğ¸ GraphQL?"
LLM Switcher (Frontend)
jsx
// ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Perplexity Ğ¸ Ollama
[ğŸŒ Perplexity] [ğŸ¤– Ollama]

// Perplexity - ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ğ´Ğ°Ñ‡/Git/Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°
// Ollama - Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ
Monitoring & Debugging
Railway Logs
bash
railway logs --service backend --tail
Vercel Logs
bash
vercel logs https://your-app.vercel.app
Ollama Logs
bash
# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°
curl http://localhost:11434

# Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
ollama list

# Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
ollama run gemma3:4b

# Ğ¢ĞµÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· API
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "gemma3:4b", "prompt": "Hello", "stream": false}'
Health Check
bash
# Backend
curl https://your-backend.railway.app/api/health

# Response: {"status": "ok", "timestamp": "2026-01-20T..."}

# Local LLM
curl http://localhost:4000/api/local-llm/health

# Response: {"status": "ok", "url": "http://localhost:11434", "model": "gemma3:4b"}
Testing
Local Testing
bash
# Backend health
curl http://localhost:4000/api/health

# Team Assistant test (Perplexity)
curl -X POST http://localhost:4000/api/team/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°"}'

# Local LLM test (Ollama)
curl -X POST http://localhost:4000/api/local-llm/ask \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ MCP?"}'
Production Testing
bash
# Frontend (Vercel)
curl https://your-app.vercel.app

# Backend (Railway)
curl https://your-backend.railway.app/api/health

# Team Assistant
curl -X POST https://your-backend.railway.app/api/team/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "ĞŸĞ¾ĞºĞ°Ğ¶Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸"}'
Troubleshooting
Railway Deployment Fails
Error: Railpack could not determine how to build

Solution:

Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Root Directory: /server (Settings â†’ Source)

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ railway.json Ñ startCommand

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ package.json Ğ¸Ğ¼ĞµĞµÑ‚ start ÑĞºÑ€Ğ¸Ğ¿Ñ‚

Error: Module not found Ğ² production

Solution:

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² package.json (Ğ½Ğµ Ğ² devDependencies)

Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ Ñ‡Ñ‚Ğ¾ npm install Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€ĞµĞ´ start

Vercel Deployment Fails
Error: Build failed

Solution:

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ vercel.json â†’ buildCommand Ğ¸ outputDirectory

Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Node.js Ğ²ĞµÑ€ÑĞ¸Ñ Ñ‡ĞµÑ€ĞµĞ· NODE_VERSION env var

Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ Ñ‡Ñ‚Ğ¾ npm run build Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾

Error: 404 on refresh

Solution:
Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ğ² vercel.json:

json
{
  "routes": [
    { "src": "/[^.]+", "dest": "/", "status": 200 }
  ]
}
Ollama Issues
Error: Connection refused (localhost:11434)

Solution:

bash
# Windows
ollama serve

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ°
curl http://localhost:11434
Error: Model not found

Solution:

bash
# Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
ollama list

# Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
ollama pull gemma3:4b

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ² ĞºĞ¾Ğ´Ğµ
# server/.env: OLLAMA_MODEL=gemma3:4b
Error: 404 on /api/generate

Solution:

ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ²ĞµÑ€ÑĞ¸Ñ Ollama: ollama --version

ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ´Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹: ÑĞºĞ°Ñ‡Ğ°Ğ¹Ñ‚Ğµ Ñ ollama.com

ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ: ollama serve

CORS Errors
Error: Access-Control-Allow-Origin

Solution (server/index.js):

javascript
app.use(cors({
  origin: process.env.NODE_ENV === 'production' 
    ? 'https://your-app.vercel.app'
    : 'http://localhost:5173'
}));
Ğ’ĞµÑ€ÑĞ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
Ğ’ĞµÑ€ÑĞ¸Ñ: v1.3.0
Ğ”Ğ°Ñ‚Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ: 2026-01-20 00:14 MSK
Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: âœ… Production Deployed + Local LLM Integrated

Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ v1.3.0:
ğŸ¤– Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM (Ollama)

âœ… server/localLlmClient.js - ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ollama API

âœ… server/teamAssistantService.js - Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM

âœ… client/src/pages/TeamAssistantPage.jsx - Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ LLM switcher

âœ… client/src/styles/TeamAssistantPage.css - ÑÑ‚Ğ¸Ğ»Ğ¸ Ğ´Ğ»Ñ switcher

âœ… ĞĞ¾Ğ²Ñ‹Ğµ API ÑĞ½Ğ´Ğ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹: /api/local-llm/*

âœ… Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ollama

âœ… ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ LLM > RAG)

ğŸ“š ĞĞ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (343 embeddings)

Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ v1.2.0:
ğŸš€ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ CI/CD Ñ‡ĞµÑ€ĞµĞ· GitHub Actions

â˜ï¸ Ğ”ĞµĞ¿Ğ»Ğ¾Ğ¹ Ğ½Ğ° Vercel (Frontend) + Railway (Backend)

ğŸ” GitHub Secrets integration

ğŸ“ Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ deployment

ğŸ”§ Railway/Vercel ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹

ğŸ› Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ CORS Ğ´Ğ»Ñ production

Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ v1.1.1:
âœ… Git MCP v1.2.1: Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚Ğ¾Ğ²

âœ… Team Assistant: Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½

âœ… Ğ’ÑĞµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ñ‹ (10/10)

Roadmap
v1.4.0 (Next Release)
â˜ï¸ Ollama Ğ² Docker Ğ´Ğ»Ñ production deployment

ğŸ“Š Vercel Analytics integration

ğŸ’¾ Railway volume Ğ´Ğ»Ñ persistent storage

ğŸ§ª Automated testing Ğ² CI/CD pipeline

ğŸ”„ Rollback mechanism Ğ´Ğ»Ñ failed deployments

âš™ï¸ Environment-specific configs (staging/production)

v1.5.0 (Future)
ğŸ¯ Streaming Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ LLM

ğŸ”„ ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ LLM (routing)

ğŸ“ˆ ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM

ğŸŒ Kubernetes deployment (alternative to Railway)

ğŸš€ Multi-region deployment

ğŸ—„ï¸ Redis caching layer

ğŸ“Š Grafana/Prometheus monitoring

Ğ›Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ñ
MIT

Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°: AI Challenge 23 - MCP Integration + RAG + CI/CD + Local LLM
Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: âœ… Production Ready (v1.3.0)
ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ: 2026-01-20 00:14 MSK
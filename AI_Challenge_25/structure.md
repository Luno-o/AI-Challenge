–í–æ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –ª–æ–∫–∞–ª—å–Ω–æ–π LLM:

text
# MCP Server & Chat Integration Project with RAG + Git + Support + Team Assistant + Local LLM

## –û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ MCP (Model Context Protocol) —Å–µ—Ä–≤–µ—Ä–æ–≤ —Å React-—á–∞—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º, **RAG (Retrieval-Augmented Generation)** —Å–∏—Å—Ç–µ–º–æ–π, **Support Assistant**, **Team Assistant** –∏ **–ª–æ–∫–∞–ª—å–Ω–æ–π LLM (Ollama)**. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∞–º–∏, –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, GitHub API, Docker, Git –æ–ø–µ—Ä–∞—Ü–∏–π, Document Indexing Pipeline, –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥–æ–π –∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤.

## Deployment Architecture

### Production Endpoints
- **Frontend (Vercel)**: `https://your-app.vercel.app`
  - React 18 + Vite
  - Serverless Functions
  - Environment: VITE_API_URL
  
- **Backend (Railway)**: `https://your-backend.railway.app`
  - Node.js Express API
  - Nixpacks builder
  - Root Directory: `/server`
  - Environment: PORT=4000 (Railway –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

- **Local LLM (Ollama)**: `http://localhost:11434`
  - –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ –º–∞—à–∏–Ω–µ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞
  - –ú–æ–¥–µ–ª–∏: gemma3:4b, llama3.2:3b, nomic-embed-text
  - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ REST API

### GitHub Actions CI/CD Pipeline

**Workflow —Ñ–∞–π–ª**: `.github/workflows/deploy.yml`

```yaml
name: Deploy to Production

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  deploy-frontend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy to Vercel
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          working-directory: ./client
          vercel-args: '--prod'
          
  deploy-backend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Deploy to Railway
        run: |
          npm install -g @railway/cli
          railway up --service backend
        env:
          RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}
          
  notify:
    needs: [deploy-frontend, deploy-backend]
    runs-on: ubuntu-latest
    steps:
      - name: Discord Notification
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ secrets.DISCORD_WEBHOOK }}
          title: "Deployment Complete"
          description: |
            ‚úÖ Frontend: ${{ secrets.VERCEL_URL }}
            ‚úÖ Backend: ${{ secrets.RAILWAY_URL }}
GitHub Secrets Configuration
Required Secrets (Settings ‚Üí Secrets and variables ‚Üí Actions):

Secret Name	Description	Source
VERCEL_TOKEN	Vercel Access Token	vercel.com ‚Üí Account Settings ‚Üí Tokens
VERCEL_ORG_ID	Vercel Organization ID	vercel.com/account ‚Üí Settings ‚Üí General ‚Üí Your ID
VERCEL_PROJECT_ID	Vercel Project ID	vercel.com/dashboard ‚Üí Project ‚Üí Settings ‚Üí Project ID
RAILWAY_TOKEN	Railway API Token	railway.app ‚Üí Account Settings ‚Üí Tokens
PERPLEXITY_API_KEY	Perplexity AI API Key	perplexity.ai/settings/api
DOCKER_USERNAME	Docker Hub Username	hub.docker.com ‚Üí Account
DOCKER_PASSWORD	Docker Hub Access Token	hub.docker.com ‚Üí Account Settings ‚Üí Security
DISCORD_WEBHOOK	Discord Webhook URL	Discord ‚Üí Channel Settings ‚Üí Integrations ‚Üí Webhooks
VERCEL_URL	Frontend Production URL	–ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –¥–µ–ø–ª–æ—è
RAILWAY_URL	Backend Production URL	–ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –¥–µ–ø–ª–æ—è
–°—Ç–µ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
–Ø–∑—ã–∫ backend: JavaScript (Node.js ESM)

–Ø–∑—ã–∫ frontend: JavaScript (React 18)

–§—Ä–µ–π–º–≤–æ—Ä–∫–∏: Express.js, React, Vite

AI/LLM:

Perplexity API (sonar model) - –æ–±–ª–∞—á–Ω–∞—è

Ollama (gemma3:4b, llama3.2:3b) - –ª–æ–∫–∞–ª—å–Ω–∞—è

MCP SDK: @modelcontextprotocol/sdk

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏: Axios, node-fetch, node-cron, dockerode

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: Docker, Docker Compose, Git

Deployment: Vercel (Frontend), Railway (Backend)

CI/CD: GitHub Actions

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
text
AI_Challenge_23/
‚îú‚îÄ‚îÄ .github/                           # üÜï GitHub Actions CI/CD
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ deploy.yml                 # Deployment pipeline
‚îÇ
‚îú‚îÄ‚îÄ server/                            # Backend (Node.js) - Railway
‚îÇ   ‚îú‚îÄ‚îÄ index.js                       # Express —Å–µ—Ä–≤–µ—Ä (PORT=4000)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ MCP Clients & Services
‚îÇ   ‚îú‚îÄ‚îÄ mcpClient.js
‚îÇ   ‚îú‚îÄ‚îÄ ragMcpClient.js
‚îÇ   ‚îú‚îÄ‚îÄ gitMcpClient.js
‚îÇ   ‚îú‚îÄ‚îÄ supportMcpClient.js
‚îÇ   ‚îú‚îÄ‚îÄ localLlmClient.js             # üÜï –ö–ª–∏–µ–Ω—Ç –¥–ª—è Ollama
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Service Layer
‚îÇ   ‚îú‚îÄ‚îÄ ragService.js
‚îÇ   ‚îú‚îÄ‚îÄ assistantService.js
‚îÇ   ‚îú‚îÄ‚îÄ supportAssistantService.js
‚îÇ   ‚îú‚îÄ‚îÄ teamAssistantService.js       # üîÑ –û–±–Ω–æ–≤–ª—ë–Ω (+ Local LLM)
‚îÇ   ‚îú‚îÄ‚îÄ documentIndexer.js
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ MCP Servers
‚îÇ   ‚îú‚îÄ‚îÄ documents-mcp.js
‚îÇ   ‚îú‚îÄ‚îÄ task-mcp-server.js
‚îÇ   ‚îú‚îÄ‚îÄ git-mcp-server.js              # v1.2.1 (Railway compatible)
‚îÇ   ‚îú‚îÄ‚îÄ docker-mcp-server.js
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Orchestration & Utils
‚îÇ   ‚îú‚îÄ‚îÄ agent-orchestrator.js
‚îÇ   ‚îú‚îÄ‚îÄ githubTools.js
‚îÇ   ‚îú‚îÄ‚îÄ githubService.js
‚îÇ   ‚îú‚îÄ‚îÄ prReviewService.js
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Data Storage
‚îÇ   ‚îú‚îÄ‚îÄ documents/                     # Markdown –¥–æ–∫—É–º–µ–Ω—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ indexes/                       # JSON –∏–Ω–¥–µ–∫—Å—ã (343 embeddings)
‚îÇ   ‚îú‚îÄ‚îÄ tasks.json                     # –ë–∞–∑–∞ –∑–∞–¥–∞—á
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Configuration
‚îÇ   ‚îú‚îÄ‚îÄ .env                           # üîÑ Railway + Ollama Environment Variables
‚îÇ   ‚îÇ   # PERPLEXITY_API_KEY (from Railway secrets)
‚îÇ   ‚îÇ   # PERPLEXITY_MODEL=sonar
‚îÇ   ‚îÇ   # OLLAMA_URL=http://localhost:11434  # üÜï
‚îÇ   ‚îÇ   # OLLAMA_MODEL=gemma3:4b             # üÜï
‚îÇ   ‚îÇ   # REPO_PATH=/app/repo (Railway volume)
‚îÇ   ‚îÇ   # PORT=4000 (Railway auto-injected)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ railway.json                   # üÜï Railway config
‚îÇ   ‚îÇ   # {"builder": "NIXPACKS", "deploy": {"startCommand": "node index.js"}}
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ client/                            # Frontend (React + Vite) - Vercel
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatPage.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AssistantPage.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SupportPage.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TeamAssistantPage.jsx  # üîÑ –û–±–Ω–æ–≤–ª—ë–Ω (+ LLM Switcher)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useTeamAssistant.js    # –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styles/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ TeamAssistantPage.css  # üîÑ –û–±–Ω–æ–≤–ª—ë–Ω (+ LLM Switcher styles)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ .env                           # üÜï Vercel Environment Variables
‚îÇ   ‚îÇ   # VITE_API_URL=https://your-backend.railway.app
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ vercel.json                    # üÜï Vercel config
‚îÇ   ‚îÇ   # {"buildCommand": "npm run build", "outputDirectory": "dist"}
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îî‚îÄ‚îÄ Configuration
    ‚îú‚îÄ‚îÄ .env.example                   # üîÑ –®–∞–±–ª–æ–Ω –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ + Ollama
    ‚îú‚îÄ‚îÄ .gitignore                     # üÜï –ò—Å–∫–ª—é—á–∏—Ç—å .env, node_modules
    ‚îú‚îÄ‚îÄ package.json
    ‚îî‚îÄ‚îÄ structure.md                   # –≠—Ç–æ—Ç —Ñ–∞–π–ª (v1.3.0)
Deployment Configuration
Railway (Backend)
–§–∞–π–ª: server/railway.json

json
{
  "builder": "NIXPACKS",
  "deploy": {
    "startCommand": "node index.js",
    "healthcheckPath": "/api/health",
    "restartPolicyType": "ON_FAILURE"
  }
}
Environment Variables (Railway Dashboard):

bash
PERPLEXITY_API_KEY=pplx-xxxxxxxxxxxx
PERPLEXITY_MODEL=sonar
REPO_PATH=/app/repo
PORT=4000  # Auto-injected by Railway
NODE_ENV=production
Root Directory: /server (Settings ‚Üí Source ‚Üí Root Directory)

Vercel (Frontend)
–§–∞–π–ª: client/vercel.json

json
{
  "buildCommand": "npm run build",
  "outputDirectory": "dist",
  "installCommand": "npm install",
  "framework": "vite",
  "routes": [
    { "src": "/[^.]+", "dest": "/", "status": 200 }
  ]
}
Environment Variables (Vercel Dashboard):

bash
VITE_API_URL=https://your-backend.railway.app
NODE_VERSION=20
Ollama (Local LLM)
–£—Å—Ç–∞–Ω–æ–≤–∫–∞ (Windows):

bash
# 1. –°–∫–∞—á–∞—Ç—å —Å ollama.com
# 2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞ localhost:11434

# 3. –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª–∏
ollama pull gemma3:4b
ollama pull llama3.2:3b
ollama pull nomic-embed-text

# 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å
ollama list
curl http://localhost:11434
Environment Variables (Local Development):

bash
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:4b
Development vs Production
Local Development
bash
# Terminal 1 - Ollama (–µ—Å–ª–∏ –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
ollama serve

# Terminal 2 - Backend
cd server
npm install
npm run dev  # http://localhost:4000

# Terminal 3 - Frontend  
cd client
npm install
npm run dev  # http://localhost:5173
Production (GitHub Actions)
bash
git add .
git commit -m "feat: deploy update"
git push origin main

# üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥–µ–ø–ª–æ–π:
# 1. GitHub Actions –∑–∞–ø—É—Å–∫–∞–µ—Ç workflow
# 2. Frontend ‚Üí Vercel (serverless)
# 3. Backend ‚Üí Railway (container)
# 4. Discord notification —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
API Endpoints (Production)
Base URL: https://your-backend.railway.app

‚úÖ Team Assistant API
Endpoint	Method	Body	Description
/api/team/ask	POST	{query, user_id}	Natural Language –∑–∞–ø—Ä–æ—Å—ã (Perplexity + Ollama)
ü§ñ Local LLM API (NEW)
Endpoint	Method	Body	Description
/api/local-llm/ask	POST	{prompt, temperature, top_p}	–ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ Ollama
/api/local-llm/health	GET	-	–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama
/api/local-llm/models	GET	-	–°–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
üìö Documents Pipeline
Endpoint	Method	Body	Description
/api/documents/index	POST	{directory, index_name}	–°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
/api/documents/search	POST	{query, index_name, top_k}	–ü–æ–∏—Å–∫
ü§ñ RAG API
Endpoint	Method	Body	Description
/api/rag/ask	POST	{question, mode, topK}	RAG –∑–∞–ø—Ä–æ—Å
üí¨ Support Assistant
Endpoint	Method	Body	Description
/api/support/ask	POST	{user_id, question}	–ü–æ–¥–¥–µ—Ä–∂–∫–∞
‚úÖ Git Assistant API
Endpoint	Method	Body	Description
/api/assistant/command	POST	{command}	Git –∫–æ–º–∞–Ω–¥—ã
Environment Variables
Development (.env.example)
bash
# Perplexity AI
PERPLEXITY_API_KEY=pplx-xxxxxxxxxxxx
PERPLEXITY_MODEL=sonar

# Server
PORT=4000

# Git MCP (Local)
REPO_PATH=D:\perplexity-chat  # Windows: –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å

# Ollama (Local LLM) üÜï
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:4b

# GitHub (optional)
GITHUB_TOKEN=ghp_xxxxxxxxxxxx
Production - Railway (Backend)
bash
PERPLEXITY_API_KEY=<from GitHub Secrets>
PERPLEXITY_MODEL=sonar
REPO_PATH=/app/repo
PORT=4000
NODE_ENV=production
# Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –≤ production (—Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω–æ)
Production - Vercel (Frontend)
bash
VITE_API_URL=https://your-backend.railway.app
NODE_VERSION=20
Team Assistant Features
üåê Perplexity Mode (Default)
‚úÖ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∞–º–∏ (—Å–æ–∑–¥–∞–Ω–∏–µ, –ø—Ä–æ—Å–º–æ—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ)

‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á —Å —É—á—ë—Ç–æ–º Git –∏–∑–º–µ–Ω–µ–Ω–∏–π

‚úÖ –°—Ç–∞—Ç—É—Å –ø—Ä–æ–µ–∫—Ç–∞ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞

‚úÖ Git –æ–ø–µ—Ä–∞—Ü–∏–∏ (status, commits, history)

‚úÖ RAG –ø–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

‚úÖ –£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

–ü—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥:

text
"–ü–æ–∫–∞–∂–∏ –≤—Å–µ –∑–∞–¥–∞—á–∏"
"–ß—Ç–æ –¥–µ–ª–∞—Ç—å –ø–µ—Ä–≤—ã–º?"
"–°—Ç–∞—Ç—É—Å –ø—Ä–æ–µ–∫—Ç–∞"
"–°–æ–∑–¥–∞–π –∑–∞–¥–∞—á—É: –∏—Å–ø—Ä–∞–≤–∏—Ç—å –±–∞–≥, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç high"
"–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç RAG –≤ —ç—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–µ?"
"–ü–æ–∫–∞–∂–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∫–æ–º–º–∏—Ç–æ–≤"
ü§ñ Ollama Mode (Local LLM)
‚úÖ –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞

‚úÖ –ü—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å (–≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω–æ)

‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞

‚úÖ –û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è

‚úÖ –ú–æ–¥–µ–ª–∏: gemma3:4b (3.3 GB), llama3.2:3b (2.0 GB)

–ê–∫—Ç–∏–≤–∞—Ü–∏—è:

–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –Ω–∞ "ü§ñ Ollama" –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ

–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –Ω–∞–ø—Ä—è–º—É—é

–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:

text
"–ß—Ç–æ —Ç–∞–∫–æ–µ MCP –ø—Ä–æ—Ç–æ–∫–æ–ª?"
"–û–±—ä—è—Å–Ω–∏ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç RAG"
"–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä?"
"–í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É REST –∏ GraphQL?"
LLM Switcher (Frontend)
jsx
// –ü–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å –º–µ–∂–¥—É Perplexity –∏ Ollama
[üåê Perplexity] [ü§ñ Ollama]

// Perplexity - —É–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á/Git/–ø—Ä–æ–µ–∫—Ç–∞
// Ollama - –±—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã, –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è
Monitoring & Debugging
Railway Logs
bash
railway logs --service backend --tail
Vercel Logs
bash
vercel logs https://your-app.vercel.app
Ollama Logs
bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
curl http://localhost:11434

# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
ollama list

# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏
ollama run gemma3:4b

# –¢–µ—Å—Ç —á–µ—Ä–µ–∑ API
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "gemma3:4b", "prompt": "Hello", "stream": false}'
Health Check
bash
# Backend
curl https://your-backend.railway.app/api/health

# Response: {"status": "ok", "timestamp": "2026-01-20T..."}

# Local LLM
curl http://localhost:4000/api/local-llm/health

# Response: {"status": "ok", "url": "http://localhost:11434", "model": "gemma3:4b"}
Testing
Local Testing
bash
# Backend health
curl http://localhost:4000/api/health

# Team Assistant test (Perplexity)
curl -X POST http://localhost:4000/api/team/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "–°—Ç–∞—Ç—É—Å –ø—Ä–æ–µ–∫—Ç–∞"}'

# Local LLM test (Ollama)
curl -X POST http://localhost:4000/api/local-llm/ask \
  -H "Content-Type: application/json" \
  -d '{"prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ MCP?"}'
Production Testing
bash
# Frontend (Vercel)
curl https://your-app.vercel.app

# Backend (Railway)
curl https://your-backend.railway.app/api/health

# Team Assistant
curl -X POST https://your-backend.railway.app/api/team/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "–ü–æ–∫–∞–∂–∏ –∑–∞–¥–∞—á–∏"}'
Troubleshooting
Railway Deployment Fails
Error: Railpack could not determine how to build

Solution:

–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Root Directory: /server (Settings ‚Üí Source)

–°–æ–∑–¥–∞–π—Ç–µ railway.json —Å startCommand

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ package.json –∏–º–µ–µ—Ç start —Å–∫—Ä–∏–ø—Ç

Error: Module not found –≤ production

Solution:

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ package.json (–Ω–µ –≤ devDependencies)

–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ npm install –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–µ—Ä–µ–¥ start

Vercel Deployment Fails
Error: Build failed

Solution:

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ vercel.json ‚Üí buildCommand –∏ outputDirectory

–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Node.js –≤–µ—Ä—Å–∏—é —á–µ—Ä–µ–∑ NODE_VERSION env var

–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ npm run build —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ

Error: 404 on refresh

Solution:
–î–æ–±–∞–≤—å—Ç–µ –≤ vercel.json:

json
{
  "routes": [
    { "src": "/[^.]+", "dest": "/", "status": 200 }
  ]
}
Ollama Issues
Error: Connection refused (localhost:11434)

Solution:

bash
# Windows
ollama serve

# –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:11434
Error: Model not found

Solution:

bash
# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
ollama list

# –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å
ollama pull gemma3:4b

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤ –∫–æ–¥–µ
# server/.env: OLLAMA_MODEL=gemma3:4b
Error: 404 on /api/generate

Solution:

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–µ—Ä—Å–∏—é Ollama: ollama --version

–û–±–Ω–æ–≤–∏—Ç–µ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π: —Å–∫–∞—á–∞–π—Ç–µ —Å ollama.com

–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve

CORS Errors
Error: Access-Control-Allow-Origin

Solution (server/index.js):

javascript
app.use(cors({
  origin: process.env.NODE_ENV === 'production' 
    ? 'https://your-app.vercel.app'
    : 'http://localhost:5173'
}));
–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
–í–µ—Ä—Å–∏—è: v1.3.0
–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: 2026-01-20 00:14 MSK
–°—Ç–∞—Ç—É—Å: ‚úÖ Production Deployed + Local LLM Integrated

–ò–∑–º–µ–Ω–µ–Ω–∏—è v1.3.0:
ü§ñ –î–æ–±–∞–≤–ª–µ–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM (Ollama)

‚úÖ server/localLlmClient.js - –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama API

‚úÖ server/teamAssistantService.js - –æ–±–Ω–æ–≤–ª—ë–Ω —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ª–æ–∫–∞–ª—å–Ω–æ–π LLM

‚úÖ client/src/pages/TeamAssistantPage.jsx - –¥–æ–±–∞–≤–ª–µ–Ω LLM switcher

‚úÖ client/src/styles/TeamAssistantPage.css - —Å—Ç–∏–ª–∏ –¥–ª—è switcher

‚úÖ –ù–æ–≤—ã–µ API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã: /api/local-llm/*

‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é Ollama

‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ (–ª–æ–∫–∞–ª—å–Ω–∞—è LLM > RAG)

üìö –û–±–Ω–æ–≤–ª—ë–Ω –∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (343 embeddings)

–ò–∑–º–µ–Ω–µ–Ω–∏—è v1.2.0:
üöÄ –î–æ–±–∞–≤–ª–µ–Ω CI/CD —á–µ—Ä–µ–∑ GitHub Actions

‚òÅÔ∏è –î–µ–ø–ª–æ–π –Ω–∞ Vercel (Frontend) + Railway (Backend)

üîê GitHub Secrets integration

üìù –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ deployment

üîß Railway/Vercel –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

üêõ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è CORS –¥–ª—è production

–ò–∑–º–µ–Ω–µ–Ω–∏—è v1.1.1:
‚úÖ Git MCP v1.2.1: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω –ø–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–º–∏—Ç–æ–≤

‚úÖ Team Assistant: –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω

‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã (10/10)


üß™ LLM Optimization (v1.4.0)
–û–ø–∏—Å–∞–Ω–∏–µ
–°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM (Ollama). –ü–æ–∑–≤–æ–ª—è–µ—Ç:

–ü—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ—Å–µ—Ç—ã

–û—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞–ø—Ä–æ—Å—ã —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (temperature, top_p, top_k, num_predict, repeat_penalty)

–°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ —à–∞–±–ª–æ–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
Frontend
–§–∞–π–ª: client/src/pages/LlmOptimizationPage.jsx

–°—Ç–∏–ª–∏: client/src/pages/LlmOptimizationPage.css

–ú–∞—Ä—à—Ä—É—Ç: /llm-optimization

Backend API
Endpoint	Method	Body	Description
/api/llm/models	GET	-	–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π, –ø—Ä–µ—Å–µ—Ç–æ–≤ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
/api/llm/optimized	POST	{prompt, temperature, top_p, top_k, num_predict, repeat_penalty, system, preset}	–ó–∞–ø—Ä–æ—Å —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
/api/llm/test-config	POST	{prompt, configs[]}	–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
/api/llm/template	POST	{template_name, data, preset}	–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞
Backend Service Layer
–§–∞–π–ª: server/localLlmClient.js

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π: server/ollamaConfig.js

js
export const OLLAMA_MODELS = {
  'gemma2:2b': { size: '1.6GB', contextWindow: 8192, ... },
  'llama3.2:3b': { size: '2.0GB', contextWindow: 4096, ... },
  'mistral:7b': { size: '4.1GB', contextWindow: 8192, ... }
};

export const TASK_PRESETS = {
  coding: { temperature: 0.3, top_p: 0.9, ... },
  creative: { temperature: 0.9, top_p: 0.95, ... },
  factual: { temperature: 0.5, top_p: 0.85, ... }
};
–®–∞–±–ª–æ–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤: server/promptTemplates.js

js
export const PROMPT_TEMPLATES = {
  task_analysis: (data) => `–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–¥–∞—á—É: ${data.task}...`,
  code_review: (data) => `–ü—Ä–æ–≤–µ—Ä—å –∫–æ–¥:\n${data.code}...`,
  explain_concept: (data) => `–û–±—ä—è—Å–Ω–∏: ${data.concept}...`
};

export const SYSTEM_PROMPTS = {
  assistant: '–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫...',
  code_expert: '–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é...',
  debugger: '–¢—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –æ—Ç–ª–∞–¥–∫–µ...'
};
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
bash
# –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–µ—Å–µ—Ç–æ–≤
curl http://localhost:4000/api/llm/models

# –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
curl -X POST http://localhost:4000/api/llm/optimized \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "–û–±—ä—è—Å–Ω–∏ MCP –ø—Ä–æ—Ç–æ–∫–æ–ª",
    "temperature": 0.7,
    "top_p": 0.9,
    "preset": "factual"
  }'

# –°—Ä–∞–≤–Ω–∏—Ç—å 3 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
curl -X POST http://localhost:4000/api/llm/test-config \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "–ß—Ç–æ —Ç–∞–∫–æ–µ Docker?",
    "configs": [
      {"temperature": 0.5, "top_p": 0.85},
      {"temperature": 0.7, "top_p": 0.9},
      {"temperature": 0.9, "top_p": 0.95}
    ]
  }'
üê≥ Docker Compose Configuration (v1.4.0)
–û–ø–∏—Å–∞–Ω–∏–µ
–õ–æ–∫–∞–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Docker Compose —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–±–æ—Ä–∫–∏ backend/frontend –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Ollama.

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
–§–∞–π–ª: docker-compose.yml

text
services:
  backend:
    build:
      context: ./server
    image: luno2/perplexity-backend:latest
    container_name: perplexity-backend
    ports:
      - "4000:4000"
    environment:
      - NODE_ENV=production
      - PORT=4000
      - PERPLEXITY_API_KEY=${PERPLEXITY_API_KEY}
      - PERPLEXITY_MODEL=${PERPLEXITY_MODEL}
      - REPO_PATH=/app
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL}
    volumes:
      - ./server/documents:/app/documents
      - ./server/indexes:/app/indexes
      - ./server/tasks.json:/app/tasks.json
      - ./server/logs:/app/logs
    depends_on:
      - ollama
    restart: unless-stopped

  frontend:
    build:
      context: ./client
    image: luno2/perplexity-frontend:latest
    container_name: perplexity-frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: perplexity-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        sleep 5
        ollama pull ${OLLAMA_MODEL:-gemma3:4b}
        wait

volumes:
  ollama-data:
Backend Dockerfile
–§–∞–π–ª: server/Dockerfile

text
FROM node:20-alpine

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å build-–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è node-gyp/sharp
RUN apk add --no-cache python3 make g++ git bash

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .

EXPOSE 4000

CMD ["node", "index.js"]
–ö–æ–º–∞–Ω–¥—ã –∑–∞–ø—É—Å–∫–∞
bash
# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
docker compose down

# –ü–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å backend/frontend –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞
docker compose build backend frontend

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å—ë
docker compose up -d backend frontend ollama

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
docker compose ps

# –õ–æ–≥–∏
docker compose logs backend --tail=50
docker compose logs ollama --tail=50
Environment Variables
–§–∞–π–ª: .env (–∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞)

bash
# Perplexity API
PERPLEXITY_API_KEY=pplx-xxxxxxxxxxxx
PERPLEXITY_MODEL=sonar

# Ollama
OLLAMA_MODEL=gemma3:4b
üåê CORS Configuration (v1.4.0)
–û–ø–∏—Å–∞–Ω–∏–µ
–ù–∞—Å—Ç—Ä–æ–π–∫–∏ CORS –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞ –Ω–∞ http://localhost (Nginx, –ø–æ—Ä—Ç 80) –∫ –±—ç–∫–µ–Ω–¥—É –Ω–∞ http://localhost:4000.

Backend CORS Setup
–§–∞–π–ª: server/index.js

js
app.use(cors({
  origin: [
    'http://localhost',           // Nginx (Docker frontend –Ω–∞ –ø–æ—Ä—Ç—É 80)
    'http://127.0.0.1',
    'http://localhost:5173',      // Vite dev server
    'http://localhost:3000',
    'http://localhost:5174',
    'http://127.0.0.1:5173',
    'http://YOUR_VPS_IP',         // Production VPS
    'https://YOUR_DOMAIN.com'     // Production domain
  ],
  methods: ['GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'OPTIONS'],
  credentials: true,
  allowedHeaders: ['Content-Type', 'Authorization']
}));
Troubleshooting CORS
–û—à–∏–±–∫–∞: Access to fetch at 'http://localhost:4000/api/llm/models' from origin 'http://localhost' has been blocked by CORS policy

–†–µ—à–µ–Ω–∏–µ:

–î–æ–±–∞–≤–∏—Ç—å http://localhost –∏ http://127.0.0.1 –≤ cors({ origin: [...] })

–ü–µ—Ä–µ—Å–æ–±—Ä–∞—Ç—å backend:

bash
docker compose build backend
docker compose up -d backend
–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤ –±—Ä–∞—É–∑–µ—Ä–µ: –æ—à–∏–±–∫–∞ –¥–æ–ª–∂–Ω–∞ –∏—Å—á–µ–∑–Ω—É—Ç—å, –∑–∞–ø—Ä–æ—Å—ã –ø—Ä–æ—Ö–æ–¥—è—Ç

–ü—Ä–æ–≤–µ—Ä–∫–∞ CORS —á–µ—Ä–µ–∑ curl:

bash
curl -X OPTIONS http://localhost:4000/api/llm/models \
  -H "Origin: http://localhost" \
  -H "Access-Control-Request-Method: GET" \
  -v
–û–∂–∏–¥–∞–µ–º—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ:

text
Access-Control-Allow-Origin: http://localhost
–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
–í–µ—Ä—Å–∏—è: v1.4.0
–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: 2026-01-23 02:06 MSK
–°—Ç–∞—Ç—É—Å: ‚úÖ Production Ready + LLM Optimization + Docker Compose

–ò–∑–º–µ–Ω–µ–Ω–∏—è v1.4.0:
üß™ LLM Optimization ‚Äî —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏ API –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π LLM

üì¶ ollamaConfig.js / promptTemplates.js ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π, –ø—Ä–µ—Å–µ—Ç–æ–≤ –∏ —à–∞–±–ª–æ–Ω–æ–≤

üê≥ Docker Compose ‚Äî –ª–æ–∫–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ backend/frontend, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Ollama

üåê CORS ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∫–∞ http://localhost –¥–ª—è —Ñ—Ä–æ–Ω—Ç–∞ –Ω–∞ Nginx (–ø–æ—Ä—Ç 80)

üîß Backend Dockerfile ‚Äî –¥–æ–±–∞–≤–ª–µ–Ω Python/g++/make –¥–ª—è —Å–±–æ—Ä–∫–∏ sharp


Roadmap
v1.4.0 (Next Release)
‚òÅÔ∏è Ollama –≤ Docker –¥–ª—è production deployment

üìä Vercel Analytics integration

üíæ Railway volume –¥–ª—è persistent storage

üß™ Automated testing –≤ CI/CD pipeline

üîÑ Rollback mechanism –¥–ª—è failed deployments

‚öôÔ∏è Environment-specific configs (staging/production)

v1.5.0 (Future)
üéØ Streaming –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π LLM

üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä LLM (routing)

üìà –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM

üåç Kubernetes deployment (alternative to Railway)

üöÄ Multi-region deployment

üóÑÔ∏è Redis caching layer

üìä Grafana/Prometheus monitoring

–õ–∏—Ü–µ–Ω–∑–∏—è
MIT

–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞: AI Challenge 23 - MCP Integration + RAG + CI/CD + Local LLM
–°—Ç–∞—Ç—É—Å: ‚úÖ Production Ready (v1.3.0)
–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: 2026-01-20 00:14 MSK
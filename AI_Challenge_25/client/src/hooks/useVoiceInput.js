import { useState, useCallback, useRef } from 'react';

/**
 * Custom hook Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Web Speech API
 * @param {Function} onResult - Callback Ð¿Ñ€Ð¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð¼ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ð¸
 * @param {Function} onError - Callback Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ
 * @param {string} language - Ð¯Ð·Ñ‹Ðº Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ ('ru-RU', 'en-US')
 */
export const useVoiceInput = (onResult, onError = null, language = 'ru-RU') => {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [error, setError] = useState(null);
  const recognitionRef = useRef(null);

  const isSupported = 'webkitSpeechRecognition' in window || 'SpeechRecognition' in window;

  const startListening = useCallback(() => {
    if (!isSupported) {
      const err = 'Ð“Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ð¹ Ð²Ð²Ð¾Ð´ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð² ÑÑ‚Ð¾Ð¼ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ðµ. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Chrome Ð¸Ð»Ð¸ Edge.';
      setError(err);
      onError?.(err);
      return;
    }

    try {
      const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
      const recognition = new SpeechRecognition();
      
      recognition.lang = language;
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      recognition.onstart = () => {
        setIsListening(true);
        setError(null);
        console.log('ðŸŽ¤ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑ‡Ð¸...');
      };

      recognition.onresult = (event) => {
        const result = event.results[0][0];
        const text = result.transcript;
        const confidence = result.confidence;
        
        console.log('âœ… Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½Ð¾:', text, `(ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: ${(confidence * 100).toFixed(1)}%)`);
        setTranscript(text);
        onResult(text, confidence);
      };

      recognition.onerror = (event) => {
        console.error('âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ:', event.error);
        setIsListening(false);
        
        let errorMessage = 'ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑ‡Ð¸';
        switch (event.error) {
          case 'no-speech':
            errorMessage = 'Ð ÐµÑ‡ÑŒ Ð½Ðµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð°. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·.';
            break;
          case 'audio-capture':
            errorMessage = 'ÐœÐ¸ÐºÑ€Ð¾Ñ„Ð¾Ð½ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð¸Ð»Ð¸ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½.';
            break;
          case 'not-allowed':
            errorMessage = 'Ð”Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ Ð·Ð°Ð¿Ñ€ÐµÑ‰Ñ‘Ð½. Ð Ð°Ð·Ñ€ÐµÑˆÐ¸Ñ‚Ðµ Ð² Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ñ… Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð°.';
            break;
          case 'network':
            errorMessage = 'ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ‚Ð¸. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ñƒ.';
            break;
        }
        
        setError(errorMessage);
        onError?.(errorMessage);
      };

      recognition.onend = () => {
        setIsListening(false);
        console.log('ðŸ”´ Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾');
      };

      recognition.start();
      recognitionRef.current = recognition;
    } catch (err) {
      console.error('âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸:', err);
      setError('ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸');
      setIsListening(false);
    }
  }, [isSupported, language, onResult, onError]);

  const stopListening = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
  }, []);

  return {
    isListening,
    transcript,
    error,
    isSupported,
    startListening,
    stopListening
  };
};

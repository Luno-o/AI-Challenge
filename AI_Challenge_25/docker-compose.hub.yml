# docker-compose.hub.yml
# v1.5.1 - Production config with Git MCP support

services:
  backend:
    image: luno2/perplexity-backend:latest
    container_name: perplexity-backend
    ports:
      - "4000:4000"
    environment:
      - NODE_ENV=production
      - PORT=4000
      - PERPLEXITY_API_KEY=${PERPLEXITY_API_KEY}
      - PERPLEXITY_MODEL=${PERPLEXITY_MODEL:-sonar}
      # üÜï Git MCP
      - REPO_PATH=/app
      # üÜï Ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gemma3:4b}
      # üÜï Voice Agent (optional)
      - WHISPER_MODEL=whisper:base
      - STT_PROVIDER=ollama
    volumes:
      - ./server/documents:/app/documents
      - ./server/indexes:/app/indexes
      - ./server/tasks.json:/app/tasks.json
      - ./server/logs:/app/logs
      # üÜï –ü—Ä–æ—Ñ–∏–ª–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
      - ./server/userProfiles:/app/userProfiles
      # üÜï Git —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (read-only)
      - ./.git:/app/.git:ro
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - perplexity-network

  frontend:
    image: luno2/perplexity-frontend:latest
    container_name: perplexity-frontend
    ports:
      - "80:80"
    environment:
      - VITE_API_URL=http://backend:4000
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - perplexity-network

  ollama:
    image: ollama/ollama:latest
    container_name: perplexity-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - perplexity-network
    # üÜï –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        sleep 10
        echo "üì• Downloading models..."
        ollama pull ${OLLAMA_MODEL:-gemma3:4b}
        ollama pull llama3.2:3b
        ollama pull nomic-embed-text
        # üÜï –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: Whisper –¥–ª—è Voice Agent (–í–∞—Ä–∏–∞–Ω—Ç B)
        # ollama pull whisper:base
        echo "‚úÖ Models ready"
        wait

networks:
  perplexity-network:
    driver: bridge

volumes:
  ollama-data:
    driver: local
